{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "dataset = load_dataset(\"nirantk/finance-pdf-vqa\", split=\"train\")\n",
    "images = dataset[\"image\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "rag_data = []\n",
    "for index, image in tqdm(enumerate(images), total=10):\n",
    "    file_path = f\"/opt/datasets/test/{index}.png\"\n",
    "    image.save(file_path)\n",
    "    rag_data.append(\n",
    "        {\n",
    "            \"id\": index,\n",
    "            \"image_path\": file_path,\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python environment setup script for COLPALI\n",
    "\"\"\"\n",
    "conda create -n colpali python=3.11.4 -y\n",
    "conda activate colpali\n",
    "pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124\n",
    "pip install transformers\n",
    "pip install colpali_engine==0.1.1\n",
    "pip install mteb\n",
    "pip install qdrant-client\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# get all files in the dir data\n",
    "files = os.listdir(\"/opt/datasets/data\")\n",
    "files = [f\"/opt/datasets/data/{f}\" for f in files if f.endswith(\".webp\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoProcessor\n",
    "from PIL import Image\n",
    "from colpali_engine.models.paligemma_colbert_architecture import ColPali\n",
    "from colpali_engine.utils.colpali_processing_utils import process_images, process_queries\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http import models as rest\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ColPali.from_pretrained(\"/opt/models/base-models/vidore/colpaligemma-3b-mix-448-base\", torch_dtype=torch.bfloat16, device_map=\"cuda\").eval()\n",
    "processor = AutoProcessor.from_pretrained(\"/opt/models/base-models/vidore/colpali\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Index Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_images_v2(processor, original_images, max_length: int = 50):\n",
    "    texts_doc = [\"Describe the image, summarised by content and company\"] * len(original_images)\n",
    "    images = [(Image.open(image)).convert(\"RGB\") for image in original_images]\n",
    "\n",
    "    batch_doc = processor(\n",
    "        text=texts_doc,\n",
    "        images=images,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=\"longest\",\n",
    "        max_length=max_length + processor.image_seq_length,\n",
    "    )\n",
    "    return batch_doc, original_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(\n",
    "    files,\n",
    "    batch_size=8,\n",
    "    shuffle=False,\n",
    "    collate_fn=lambda x: process_images_v2(processor, x),\n",
    ")\n",
    "\n",
    "batch_doc_sample, images_sample = next(iter(dataloader))\n",
    "with torch.no_grad():\n",
    "    batch_doc_sample = {k: v.to(model.device) for k, v in batch_doc_sample.items()}\n",
    "    embeddings_doc_sample = model(**batch_doc_sample)\n",
    "embedding_dim = embeddings_doc_sample.shape[-1]\n",
    "\n",
    "print(f\"Embedding Dimension: {embedding_dim}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = QdrantClient(\"http://localhost:6333\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.recreate_collection(\n",
    "    collection_name=\"rag_test_2\",\n",
    "    vectors_config=rest.VectorParams(size=embedding_dim, distance=\"Cosine\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from qdrant_client.http import models\n",
    "from uuid import uuid4\n",
    "\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    files,\n",
    "    batch_size=8,\n",
    "    shuffle=False,\n",
    "    collate_fn=lambda x: process_images_v2(processor, x),\n",
    ")\n",
    "\n",
    "for batch_doc, original_images in tqdm(dataloader):\n",
    "    with torch.no_grad():\n",
    "        batch_doc = {k: v.to(model.device) for k, v in batch_doc.items()}\n",
    "        embeddings_doc = model(**batch_doc)\n",
    "    vectors = list(torch.unbind(embeddings_doc.to(\"cpu\")))\n",
    "    ids = original_images\n",
    "    payloads = [{\"doc_id\": idx} for idx in ids]\n",
    "\n",
    "    points: List[models.PointStruct] = []\n",
    "    for idx, vector, payload in zip(ids, vectors, payloads):\n",
    "        for ind, vec in enumerate(vector):\n",
    "            point_id = str(uuid4())\n",
    "            point = models.PointStruct(\n",
    "                id=point_id,\n",
    "                vector=vec.tolist(),\n",
    "                payload=payload,\n",
    "            )\n",
    "            points.append(point)\n",
    "    print(points[0])\n",
    "    client.upsert(\n",
    "        collection_name=\"rag_test_2\",\n",
    "        points=points,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "\n",
    "\n",
    "client = QdrantClient(\"http://localhost:6333\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = [\"What benefits does OpenFaaS serverless compute offer in terms of performance?\",\n",
    "\"How are cash flows reported in the E2E Cloud's financial statements according to the indirect method?\",\n",
    "\"What benefits do Kubernetes Containers provide in cloud computing environments?\",\n",
    "\"What are the cash and cash equivalents reported by E2E Cloud as of March 31, 2024, and March 31, 2023?\",\n",
    "\"Why are Internet Protocol (IP) addresses considered assets with an indefinite useful life?\",\n",
    "\"How do K8s containers boost serverless in cloud?\",\n",
    "\"How does OpenFaaS boost K8s container performance?\",\n",
    "\"How does E2E Cloud's cash flow report adjust for non-cash items and credit issues?\",\n",
    "\"How are foreign exchange gains or losses on financial liabilities denominated in a foreign currency recognized in the financial statements of E2E Networks Limited?\"]\n",
    "\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    queries,\n",
    "    batch_size=4,\n",
    "    shuffle=False,\n",
    "    collate_fn=lambda x: process_queries(processor, x, Image.new(\"RGB\", (448, 448), (255, 255, 255))),\n",
    ")\n",
    "\n",
    "results = []\n",
    "for batch_query in tqdm(dataloader):\n",
    "    with torch.no_grad():\n",
    "        batch_query = {k: v.to(model.device) for k, v in batch_query.items()}\n",
    "        embeddings_query = model(**batch_query)\n",
    "        embeddings_query_pooled = embeddings_query.mean(dim=1)\n",
    "\n",
    "    vectors = list(torch.unbind(embeddings_query_pooled.to(\"cpu\")))\n",
    "    for vector in vectors:\n",
    "        search_result = client.search(\n",
    "            collection_name=\"rag_test_2\",\n",
    "            query_vector=vector,\n",
    "            limit=10,\n",
    "        )\n",
    "        print(\"Search Results:\")\n",
    "        for hit in search_result:\n",
    "            print(f\"Document ID: {hit.payload['doc_id']}, Score: {hit.score}\")\n",
    "            results.append(hit.payload[\"doc_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image.open(\"/opt/datasets/data/12.webp\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "colpali",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
