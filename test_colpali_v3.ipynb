{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "dataset = load_dataset(\"nirantk/finance-pdf-vqa\", split=\"train\")\n",
    "images = dataset[\"image\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_data = []\n",
    "for index, image in tqdm(enumerate(images), total=len(images)):\n",
    "    file_path = f\"data/{index}.webp\"\n",
    "    image.save(file_path)\n",
    "    rag_data.append(\n",
    "        {\n",
    "            \"id\": index,\n",
    "            \"image_path\": file_path,\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python environment setup script for COLPALI\n",
    "\"\"\"\n",
    "conda create -n colpali python=3.11.4 -y\n",
    "conda activate colpali\n",
    "pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124\n",
    "pip install transformers\n",
    "pip install colpali_engine==0.1.1\n",
    "pip install mteb\n",
    "pip install qdrant-client\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "# get all files in the dir data\n",
    "files = os.listdir(\"data\")\n",
    "files = [f\"data/{f}\" for f in files if f.endswith(\".webp\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoProcessor\n",
    "from PIL import Image\n",
    "from colpali_engine.models.paligemma_colbert_architecture import ColPali\n",
    "from colpali_engine.utils.colpali_processing_utils import process_images, process_queries\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http import models as rest\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"vidore/colpali\"\n",
    "model = ColPali.from_pretrained(\n",
    "    \"vidore/colpaligemma-3b-mix-448-base\", torch_dtype=torch.bfloat16, device_map=\"cuda\"\n",
    ").eval()\n",
    "model.load_adapter(model_name)\n",
    "processor = AutoProcessor.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Index Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_images_v2(processor, original_images, max_length: int = 50):\n",
    "    texts_doc = [\"Describe the image.\"] * len(original_images)\n",
    "    images = [(Image.open(image)).convert(\"RGB\") for image in original_images]\n",
    "\n",
    "    batch_doc = processor(\n",
    "        text=texts_doc,\n",
    "        images=images,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=\"longest\",\n",
    "        max_length=max_length + processor.image_seq_length,\n",
    "    )\n",
    "    return batch_doc, original_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(\n",
    "    files,\n",
    "    batch_size=8,\n",
    "    shuffle=False,\n",
    "    collate_fn=lambda x: process_images_v2(processor, x),\n",
    ")\n",
    "\n",
    "batch_doc_sample, images_sample = next(iter(dataloader))\n",
    "with torch.no_grad():\n",
    "    batch_doc_sample = {k: v.to(model.device) for k, v in batch_doc_sample.items()}\n",
    "    embeddings_doc_sample = model(**batch_doc_sample)\n",
    "embedding_dim = embeddings_doc_sample.shape[-1]\n",
    "\n",
    "print(f\"Embedding Dimension: {embedding_dim}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = QdrantClient(\"http://localhost:6333\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.recreate_collection(\n",
    "    collection_name=\"rag_test\",\n",
    "    vectors_config=rest.VectorParams(size=embedding_dim, distance=\"Cosine\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from qdrant_client.http import models\n",
    "from uuid import uuid4\n",
    "\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    files,\n",
    "    batch_size=8,\n",
    "    shuffle=False,\n",
    "    collate_fn=lambda x: process_images_v2(processor, x),\n",
    ")\n",
    "\n",
    "for batch_doc, original_images in tqdm(dataloader):\n",
    "    with torch.no_grad():\n",
    "        batch_doc = {k: v.to(model.device) for k, v in batch_doc.items()}\n",
    "        embeddings_doc = model(**batch_doc)\n",
    "    vectors = list(torch.unbind(embeddings_doc.to(\"cpu\")))\n",
    "    ids = original_images\n",
    "    payloads = [{\"doc_id\": idx} for idx in ids]\n",
    "\n",
    "    points: List[models.PointStruct] = []\n",
    "    for idx, vector, payload in zip(ids, vectors, payloads):\n",
    "        for ind, vec in enumerate(vector):\n",
    "            point_id = str(uuid4())\n",
    "            point = models.PointStruct(\n",
    "                id=point_id,\n",
    "                vector=vec.tolist(),\n",
    "                payload=payload,\n",
    "            )\n",
    "            points.append(point)\n",
    "    print(points[0])\n",
    "    client.upsert(\n",
    "        collection_name=\"rag_test\",\n",
    "        points=points,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "\n",
    "\n",
    "client = QdrantClient(\"http://localhost:6333\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = [\"What is the total profit of E2E Networks?\"]\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    queries,\n",
    "    batch_size=4,\n",
    "    shuffle=False,\n",
    "    collate_fn=lambda x: process_queries(processor, x, Image.new(\"RGB\", (448, 448), (255, 255, 255))),\n",
    ")\n",
    "\n",
    "results = []\n",
    "for batch_query in tqdm(dataloader):\n",
    "    with torch.no_grad():\n",
    "        batch_query = {k: v.to(model.device) for k, v in batch_query.items()}\n",
    "        embeddings_query = model(**batch_query)\n",
    "        embeddings_query_pooled = embeddings_query.mean(dim=1)\n",
    "\n",
    "    vectors = list(torch.unbind(embeddings_query_pooled.to(\"cpu\")))\n",
    "    for vector in vectors:\n",
    "        search_result = client.search(\n",
    "            collection_name=\"rag_test\",\n",
    "            query_vector=vector,\n",
    "            limit=10,\n",
    "        )\n",
    "        print(\"Search Results:\")\n",
    "        for hit in search_result:\n",
    "            print(f\"Document ID: {hit.payload['doc_id']}, Score: {hit.score}\")\n",
    "            results.append(hit.payload[\"doc_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image.open(results[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "colpali",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
